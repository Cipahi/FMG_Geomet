Geometallurgical block model -The aim of the work was to develop an algorithm for the selection of the optimal coal blends based in the cross disciplines data
This model developed as linear blending models were not suitable due to the fact that coal and geomet properties are not in a perfect linear relationship.  Coal blending refers to the process of mixing or combining different coals grades that are mined from different seam locations to achieve the desired quality attributes for salable products without penalties
Geostatistics and conditional simulation (provides multiple spatial variation outcomes for scenarios for each seam) were applied to measure and quantify spatial variation of coal qualities (grade) for Ash, Moistures and calorific value for each coal seam as provided in data.  The dummy data have been prepared for classifying product based on the feature considered in this model.
Mine Optimization: Geostats and conditional simulation resulted improved classification of coal grade/waste in each roof and floor of all seams which has improved reserve calculation reflecting actual with short term scheduling pit optimization and minimise the the overall dilution compared to the previous model.
Plant optimization: geometallurgical variables are not necessarily linear or additive and therefore require very careful geostatistical and simulation scenarios to model non-linear. This Identifies the root cause of measured differences in processing behaviour and classify correct classification of products.   
The are other feature can be included when interacting with other disciplines from minerology textures, geomet textures, environmental, coal price, airflow rate, solid percentage (%) but very limited dummy features are considered which may not make sense making a final decision based on confusion matric.  This python script using latest tech is a example to illustrate that Machine lerning and Neural netwwork can be applied to geometallurgical data to solve geomet challenges of coal blending and concentrator performances.
XGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms. It uses the gradient boosting (GBM) framework at its core.						
XGBoost has perfromed better than compared to other models in classifying correct product specifications based on the given features. 

For developing XGBoost models to predict the CF metallurgical responses, from the whole dataset 80% of records were randomly considered for the training set, and the rest of the data was used for the modelâ€™s testing phase. The four main XGBoost hyperparameters are general, booster, learning task, and command-line parameters. General parameters are the overall functioning of the XGBoost model and include booster, verbosity, and nthread. From booster parameters, gbtree, gblinear, or dart can be selected. The verbosity of printing messages could be 0 (silent), 1 (warning), 2 (info), 3 (debug). The nthread can be used for parallel processing. After the tuning process (a try and error procedure), the optimum model features are adjusted and applied for training. For comparison purposes, the same training and testing sets were considered to develop RF and SVR modelsalong with other models. The findings indicated that the XGBoost model provided significantly higher accuracy for predicting classification responses than others. 
